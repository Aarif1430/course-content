{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CIS-522/course-content/blob/main/tutorials/W3_MLPs/student/W3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS-522 Week 3 Part 2\r\n",
    "# Multi-Layer Perceptrons (MLPs)\r\n",
    "\r\n",
    "__Instructor__: Konrad Kording\r\n",
    "\r\n",
    "__Content creators:__ Arash Ash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "# Tutorial Objectives\r\n",
    "In this tutorial, we delve deeper by using one of the most famous deep learning models of all!\r\n",
    "\r\n",
    "MLPs are arguably one of the most tractable models that we can use to study deep learning fundamentals. Here we will learn why MLPs are: \r\n",
    "\r\n",
    "* similar to biological networks\r\n",
    "* good at function approximation\r\n",
    "* can evolve linearly in weights \r\n",
    "* the case of deep vs. wide\r\n",
    "* dependant on transfer functions\r\n",
    "* sensitive to initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
    "my_pennkey = 'value' #@param {type:\"string\"}\n",
    "my_pod = 'Select' #@param ['Select', 'euclidean-wombat', 'sublime-newt', 'buoyant-unicorn', 'lackadaisical-manatee','indelible-stingray','superfluous-lyrebird','discreet-reindeer','quizzical-goldfish','astute-jellyfish','ubiquitous-cheetah','nonchalant-crocodile','fashionable-lemur','spiffy-eagle','electric-emu','quotidian-lion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Slides\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSPvHqDTmMq4GyQy6lieNEFxq4qz1SmqC2RNoeei3_niECH53zneh8jJVYOnBIdk0Uaz7y2b9DK8V1t/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"480\" height=\"299\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "Meet with your pod for 10 minutes to discuss what you learned, what was clear, and what you hope to learn more about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown Tell us your thoughts about what you have learned.\n",
    "my_w2_upshot = '' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dev, torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Seeding for reproducibility\n",
    "seed = 522\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_deterministic(True)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = seed % (worker_id+1)\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Dataset download\n",
    "%%capture\n",
    "!rm -r AnimalFaces32x32/\n",
    "!git clone https://github.com/arashash/AnimalFaces32x32\n",
    "!rm -r afhq/\n",
    "!unzip ./AnimalFaces32x32/afhq_32x32.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "fig_w, fig_h = (8, 6)\n",
    "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "my_layout = widgets.Layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "\n",
    "def progress(epoch, loss, epochs=100):\n",
    "    return HTML(\"\"\"\n",
    "        <label for=\"file\">Training loss: {loss}</label>\n",
    "        <progress\n",
    "            value='{epoch}'\n",
    "            max='{epochs}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {epoch}\n",
    "        </progress>\n",
    "    \"\"\".format(loss=loss, epoch=epoch, epochs=epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Part 1 Code\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, actv, num_inputs, hidden_units, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        exec('self.actv = nn.%s'%actv)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_units)):\n",
    "          next_num_inputs = hidden_units[i]\n",
    "          self.layers += [nn.Linear(num_inputs, next_num_inputs)]\n",
    "          num_inputs = next_num_inputs\n",
    "\n",
    "        self.out = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flattening\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "          x = self.actv(layer(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "K = 4\n",
    "sigma = 0.4\n",
    "N = 1000\n",
    "t = torch.linspace(0, 1, N)\n",
    "X = torch.zeros(K*N, 2)\n",
    "y = torch.zeros(K*N)\n",
    "for k in range(K):\n",
    "  X[k*N:(k+1)*N, 0] = t*(torch.sin(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))   # [TO-DO]\n",
    "  X[k*N:(k+1)*N, 1] = t*(torch.cos(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))   # [TO-DO]\n",
    "  y[k*N:(k+1)*N] = k\n",
    "\n",
    "# Shuffling\n",
    "shuffled_indeces = torch.randperm(K*N)\n",
    "X = X[shuffled_indeces]\n",
    "y = y[shuffled_indeces]\n",
    "\n",
    "# Test Train splitting\n",
    "test_size = int(0.2*N)\n",
    "X_test = X[:test_size]\n",
    "y_test = y[:test_size]\n",
    "X_train = X[test_size:]\n",
    "y_train = y[test_size:]\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=0)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True,\n",
    "                        shuffle=True, num_workers=0, worker_init_fn=seed_worker)\n",
    "\n",
    "def train_test_classification(net, criterion, optimizer,\n",
    "                              train_loader, test_loader,\n",
    "                              num_epochs=1, verbose=True,\n",
    "                              training_plot=False):\n",
    "  if verbose:\n",
    "    progress_bar = display(progress(0, 0, num_epochs), display_id=True)\n",
    "\n",
    "  net.train()\n",
    "  training_losses = []\n",
    "  for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "      running_loss = 0.0\n",
    "      for i, data in enumerate(train_loader, 0):\n",
    "          # get the inputs; data is a list of [inputs, labels]\n",
    "          inputs, labels = data\n",
    "          inputs = inputs.to(dev).float()\n",
    "          labels = labels.to(dev).long()\n",
    "\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # forward + backward + optimize\n",
    "          outputs = net(inputs)\n",
    "\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # print statistics\n",
    "          if verbose:\n",
    "            training_losses += [loss.item()]\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # update every 10 mini-batches\n",
    "                progress_bar.update(progress(epoch+1, running_loss / 10, num_epochs))\n",
    "                running_loss = 0.0\n",
    "\n",
    "  net.eval()\n",
    "  def test(data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(dev).float()\n",
    "        labels = labels.to(dev).long()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total, acc\n",
    "\n",
    "  train_total, train_acc = test(train_loader)\n",
    "  test_total, test_acc = test(test_loader)\n",
    "\n",
    "  if verbose:\n",
    "    print('Accuracy on the %d training samples: %0.2f %%' % (train_total, train_acc))\n",
    "    print('Accuracy on the %d testing samples: %0.2f %%' % (test_total, test_acc))\n",
    "\n",
    "  if training_plot:\n",
    "    plt.plot(training_losses)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.show()\n",
    "\n",
    "  return train_acc, test_acc\n",
    "\n",
    "def sample_grid(M=500, x_max = 2.0):\n",
    "  ii, jj = torch.meshgrid(torch.linspace(-x_max, x_max,M),\n",
    "                          torch.linspace(-x_max, x_max, M))\n",
    "  X_all = torch.cat([ii.unsqueeze(-1),\n",
    "                     jj.unsqueeze(-1)],\n",
    "                     dim=-1).view(-1, 2)\n",
    "  return X_all\n",
    "\n",
    "def plot_decision_map(X_all, y_pred, X_test, y_test, M=500, x_max = 2.0, eps = 1e-3):\n",
    "  decision_map = torch.argmax(y_pred, dim=1)    # [TO-DO]\n",
    "\n",
    "  for i in range(len(X_test)):\n",
    "    indeces = (X_all[:, 0] - X_test[i, 0])**2 + (X_all[:, 1] - X_test[i, 1])**2 < eps    # [TO-DO]\n",
    "    decision_map[indeces] = (K + y_test[i]).long()    # [TO-DO]\n",
    "\n",
    "  decision_map = decision_map.view(M, M).cpu()\n",
    "  plt.imshow(decision_map, extent=[-x_max, x_max, -x_max, x_max], cmap='jet')\n",
    "  plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9: Implement gradient visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polytopes(grad, M=500, x_max=1):\n",
    "  grad = grad.detach().cpu()\n",
    "  grad_colors = grad[:, 0]     # [TO-DO]\n",
    "  grad_colors = (grad_colors / grad_colors.max() * 1e3).int() % 10     # [TO-DO]\n",
    "  grad_colors = grad_colors.view(M, M).cpu().numpy()\n",
    "  plt.imshow(grad_colors, cmap='rainbow')\n",
    "  plt.show()\n",
    "\n",
    "rand_net = Net('ReLU()', X_train.shape[1], [128], K).to(dev)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "X_all = sample_grid(x_max=1)\n",
    "labels = torch.zeros(len(X_all)).long()\n",
    "X_all.requires_grad = True    # [TO-DO]\n",
    "outputs = rand_net(X_all)\n",
    "\n",
    "loss = torch.mean(outputs)    # [TO-DO]\n",
    "# loss = torch.mean(outputs**2) # try this to see how it become non-linear\n",
    "loss.backward()\n",
    "\n",
    "plot_polytopes(X_all.grad)    # [TO-DO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient visualization that flows! (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad(X_all, grad, y_test, M=500):\n",
    "  grad = grad.detach().cpu()\n",
    "  X_all = X_all.detach().cpu()\n",
    "\n",
    "  plt.quiver(X_all[:, 0], X_all[:, 1],\n",
    "             grad[:, 0], grad[:, 1], y_test)    # [TO-DO]\n",
    "  plt.show()\n",
    "\n",
    "rand_net = Net('ReLU()', X_train.shape[1], [128], K).to(dev)\n",
    "X_test.requires_grad = True    # [TO-DO]\n",
    "outputs = rand_net(X_test)\n",
    "\n",
    "loss = criterion(outputs, y_test.long())    # [TO-DO]\n",
    "loss.backward()\n",
    "\n",
    "plot_grad(X_test, X_test.grad, y_test)    # [TO-DO]\n",
    "X_test.requires_grad = False    # [TO-DO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPs vs Linear model with Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10: Add polynomial features and train without any hidden layers\r\n",
    "[Outline the goal and steps and the formulas, especially how the num_features is calculated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degree = 50\n",
    "def make_poly_features(poly_degree, X):\n",
    "  num_features = (poly_degree+1)*(poly_degree+2)//2-1   # [TO-DO]\n",
    "  poly_X = torch.zeros((X.shape[0], num_features))\n",
    "  count = 0\n",
    "  for i in range(poly_degree+1):\n",
    "    for j in range(poly_degree+1):\n",
    "      if j+i > 0: # no need to add zero degree since model has biases\n",
    "        if j+i <= poly_degree:\n",
    "          poly_X[:, count] = X[:, 0]**i * X[:, 1]**j   # [TO-DO]\n",
    "          count += 1\n",
    "  return poly_X\n",
    "\n",
    "poly_X_test = make_poly_features(poly_degree, X_test)\n",
    "poly_X_train = make_poly_features(poly_degree, X_train)\n",
    "\n",
    "batch_size = 128\n",
    "poly_test_data = TensorDataset(poly_X_test, y_test)\n",
    "poly_test_loader = DataLoader(poly_test_data, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=1)\n",
    "poly_train_data = TensorDataset(poly_X_train, y_train)\n",
    "poly_train_loader = DataLoader(poly_train_data, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_net = Net('ReLU()', poly_X_train.shape[1], [], K).to(dev)    # [TO-DO]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(poly_net.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "_, _ = train_test_classification(poly_net, criterion, optimizer,\n",
    "                                 poly_train_loader, poly_test_loader,\n",
    "                                 num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = sample_grid()\n",
    "poly_X_all = make_poly_features(poly_degree, X_all)\n",
    "y_pred = poly_net(poly_X_all)\n",
    "plot_decision_map(X_all, y_pred, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wider vs deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11: Wider vs. Deeper while keeping number of parameters same\r\n",
    "Let's find the optimal number of hidden layers under the constrained fixed number of parameters!\r\n",
    "\r\n",
    "But first, let's implement a model parameter counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "  # facny implementation\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_parameters(model):\n",
    "  # more didactic implementation\n",
    "  par_count = 0\n",
    "  for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "      par_count += p.numel()     # [TO-DO]\n",
    "  return par_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_par_count = 100\n",
    "max_hidden_layer = 5\n",
    "hidden_layers = range(1, max_hidden_layer+1)    # [TO-DO]\n",
    "test_scores = []\n",
    "for hidden_layer in hidden_layers:\n",
    "  hidden_units = np.ones(hidden_layer, dtype=np.int)    # [TO-DO]\n",
    "  wide_net = Net('ReLU()', X_train.shape[1], hidden_units, K).to(dev)\n",
    "  par_count = count_parameters(wide_net)\n",
    "  while par_count < max_par_count:\n",
    "    hidden_units += 1\n",
    "    wide_net = Net('ReLU()', X_train.shape[1], hidden_units, K).to(dev)\n",
    "    par_count = count_parameters(wide_net)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(wide_net.parameters(), lr=1e-3)\n",
    "  num_epochs = 100\n",
    "  _, test_acc = train_test_classification(wide_net, criterion, optimizer, train_loader,\n",
    "                                          test_loader, num_epochs=num_epochs)    # [TO-DO]\n",
    "  test_scores += [test_acc]\n",
    "\n",
    "plt.xlabel('# of hidden layers')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.plot(hidden_layers, test_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Tangent Kernels (NTKs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12: Motivation for NTKs\r\n",
    "lazy training of overcomplete MLPs results in linear changes in weights. Let's try to see it here,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net('ReLU()', X_train.shape[1], [1000], K).to(dev)    # [TO-DO]\n",
    "criterion = nn.CrossEntropyLoss()    # [TO-DO]\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2)    # [TO-DO]\n",
    "num_select_weights = 10\n",
    "num_time_steps = 5\n",
    "step_epoch = 40\n",
    "weights = torch.zeros(num_time_steps, num_select_weights)\n",
    "for i in range(num_time_steps):\n",
    "  _, _ = train_test_classification(net, criterion, optimizer, train_loader,\n",
    "                                  test_loader, num_epochs=step_epoch, verbose=False)\n",
    "  weights[i] = net.layers[0].weight[:num_select_weights, 0]    # [TO-DO]\n",
    "\n",
    "for k in range(num_select_weights):\n",
    "  weight = weights[:, k].detach()    # [TO-DO]\n",
    "  epochs = range(1, 1+num_time_steps*step_epoch, step_epoch)\n",
    "  plt.plot(epochs, weight, label='weight #%d'%k)\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "_, _ = train_test_classification(net, criterion, optimizer, train_loader, test_loader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper MLPs\r\n",
    "[The ability of deeper MLP to approximate a broader set of functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13: Classification on a real world dataset\r\n",
    "[Outline the goal and steps, introducing the need for augmentation, preprocessing (bring to -1,1 range), small batch_size (due to overfitting), multithreading in data loaders]\r\n",
    "[Ask to choose a good choice for augmentation and preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "batch_size = 128\n",
    "train_transform = transforms.Compose([\n",
    "     transforms.RandomRotation(10), # [TO-DO]\n",
    "     transforms.RandomHorizontalFlip(),    # [TO-DO]\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    # [TO-DO] example of a simple one\n",
    "     ])\n",
    "\n",
    "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
    "img_train_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
    "# num_workers can be set to 10 if running on Colab Pro TPUs\n",
    "img_train_loader = DataLoader(img_train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=10, worker_init_fn=seed_worker)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    # [TO-DO]\n",
    "     ])\n",
    "img_test_dataset = ImageFolder(data_path/'val', transform=test_transform)\n",
    "img_test_loader = DataLoader(img_test_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=1)\n",
    "classes = ('cat', 'dog', 'wild')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(img_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images, nrow=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net('ReLU()', 3*32*32, [128, 32], 3).to(dev)\n",
    "criterion = nn.MultiMarginLoss(margin=1.0)    # [TO-DO]\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "_, _ = train_test_classification(net, criterion, optimizer,\n",
    "                                img_train_loader, img_test_loader,\n",
    "                                num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weights = net.layers[0].weight.view(128, 3, 32, 32).detach().cpu()     # [TO-DO]\n",
    "fc1_weights /= torch.max(torch.abs(fc1_weights))\n",
    "imshow(make_grid(fc1_weights, nrow=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The choice of transfer function matters\r\n",
    "[introduce different properties of different transfer functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 15:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 14: Find the best transfer function for this model\r\n",
    "[categorizing Pytorch transfer functions according to their properties in a table so that students could have a better educated guess on which one to pick instead of trying all!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Activations\n",
    "Activations = ['ReLU', 'Tanh', 'Sigmoid', 'ELU', 'Hardshrink', 'Hardsigmoid',\n",
    "'Hardtanh', 'Hardswish', 'LeakyReLU', 'LogSigmoid', 'PReLU',\n",
    "'ReLU6', 'RReLU', 'SELU', 'CELU', 'GELU', 'SiLU', 'Softplus',\n",
    "'Softshrink', 'Softsign', 'Tanhshrink']\n",
    "\n",
    "your_picks = ['Hardswish'] # [TO-DO] other picks above 91.5% test accuracy is acceptable\n",
    "\n",
    "for actv in your_picks:\n",
    "  print(actv)\n",
    "  actv = actv+'()'\n",
    "  net = Net(actv, 3*32*32, [128, 32], 3).to(dev)\n",
    "  criterion = nn.MultiMarginLoss(margin=1.0)\n",
    "  optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
    "  _, _ = train_test_classification(net, criterion, optimizer,\n",
    "                                  img_train_loader, img_test_loader,\n",
    "                                  num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weights = net.layers[0].weight.view(128, 3, 32, 32).detach().cpu()\n",
    "fc1_weights /= torch.max(torch.abs(fc1_weights))\n",
    "imshow(make_grid(fc1_weights, nrow=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for good initialization\r\n",
    "[The discussion about why optimal gain is dependent with the transfer functions and what is theoretical optimal solution for Leaky ReLU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization\r\n",
    "Let us look at the scale distribution of an output (e.g., a hidden variable)  $o_i$  for some fully-connected layer without nonlinearities. With  $n_{in}$  inputs  ($x_j$)  and their associated weights  $w_{ij}$  for this layer. Then an output is given by,\r\n",
    "$$\r\n",
    "o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j\r\n",
    "$$\r\n",
    "The weights  $w_{ij}$  are all drawn independently from the same distribution. Furthermore, let us assume that this distribution has zero mean and variance  $\\sigma^2$ . Note that this does not mean that the distribution has to be Gaussian, just that the mean and variance need to exist. For now, let us assume that the inputs to the layer  $x_j$ also have zero mean and variance  $\\gamma^2$  and that they are independent of $w_{ij}$ and independent of each other. In this case, we can compute the mean and variance of $o_i$ as follows:\r\n",
    "\\begin{split}\\begin{aligned}\r\n",
    "    E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] = 0, \\\\\r\n",
    "    \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] = n_\\mathrm{in} \\sigma^2 \\gamma^2\r\n",
    "\\end{aligned}\\end{split}\r\n",
    "One way to keep the variance fixed is to set $n_{in}\\sigma^2=1$ . Now consider backpropagation. There we face a similar problem, albeit with gradients being propagated from the layers closer to the output. Using the same reasoning as for forward propagation, we see that the gradients’ variance can blow up unless $n_{out}\\sigma^2=1$ , where  $n_{out}$ is the number of outputs of this layer. This leaves us in a dilemma: we cannot possibly satisfy both conditions simultaneously. Instead, we simply try to satisfy:\r\n",
    "\\begin{aligned}\r\n",
    "\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or equivalently }\r\n",
    "\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}\r\n",
    "\\end{aligned}\r\n",
    "This is the reasoning underlying the now-standard and practically beneficial Xavier initialization, named after the first author of its creators [Glorot & Bengio, 2010]. Typically, the Xavier initialization samples weights from a Gaussian distribution with zero mean and variance  $\\sigma^2=\\frac{2}{(n_{in}+n_{out})}$. We can also adapt Xavier’s intuition to choose the variance when sampling weights from a uniform distribution. Note that the uniform distribution $U(−a,a)$ has variance $\\frac{a^2}{3}$. Plugging this into our condition on $\\sigma^2$ yields the suggestion to initialize according to\r\n",
    "$$\r\n",
    "U\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right)\r\n",
    "$$\r\n",
    "This explanation is mainly taken from [here](https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization with transfer function\r\n",
    "Let's derive the optimal gain for LeakyReLU following a similar steps,\r\n",
    "\r\n",
    "$$\r\n",
    "f(x)=\\left\\{\\begin{array}{ll}\r\n",
    "a x & \\text { for } x<0 \\\\\r\n",
    "x & \\text { for } x \\geq 0\r\n",
    "\\end{array}\\right.\r\n",
    "$$\r\n",
    "\r\n",
    "Considering a single layer with activation gives, \r\n",
    "\r\n",
    "\r\n",
    "The expectation of the output is still zero but the variance changes and assuming the probability $P(x < 0) = 0.5$\r\n",
    "\r\n",
    "\\begin{split}\\begin{aligned}\r\n",
    "    \\mathrm{Var}[f(o_i)] = E[f(o_i)^2] & = \\frac{\\mathrm{Var}[o_i] + a^2 \\mathrm{Var}[o_i]}{2} = \\frac{1+a^2}{2}n_\\mathrm{in} \\sigma^2 \\gamma^2\r\n",
    "\\end{aligned}\\end{split}\r\n",
    "\r\n",
    "Therefore following the rest of derivation as before,\r\n",
    "\r\n",
    "$$\r\n",
    "\\sigma = gain\\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\quad gain = \\sqrt{\\frac{2}{1+a^2}}\r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 15: Best gain for Xavier Initialization with Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 # number of trials\n",
    "gains = np.linspace(1/N, 3.0, N)\n",
    "test_accs = []\n",
    "train_accs = []\n",
    "for gain in gains:\n",
    "\n",
    "  def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain) # [TO-DO]\n",
    "        # torch.nn.init.xavier_uniform_(m.weight, gain)\n",
    "\n",
    "  negative_slope = 0.1\n",
    "  actv = 'LeakyReLU(%f)'%negative_slope # [TO-DO]\n",
    "  net = Net(actv, 3*32*32, [128, 64, 32], 3).to(dev)\n",
    "  net.apply(init_weights) # [TO-DO]\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # criterion = nn.MultiMarginLoss(margin=1.0)\n",
    "  optimizer = optim.SGD(net.parameters(), lr=1e-2) # [TO-DO]\n",
    "  train_acc, test_acc = train_test_classification(net, criterion, optimizer,\n",
    "                                                  img_train_loader, img_test_loader,\n",
    "                                                  num_epochs=1, verbose=False)\n",
    "  test_accs += [test_acc]\n",
    "  train_accs += [train_acc]\n",
    "\n",
    "best_gain = gains[np.argmax(train_accs)]\n",
    "plt.plot(gains, test_accs, label='Test')\n",
    "plt.plot(gains, train_accs, label='Train')\n",
    "plt.scatter(best_gain, max(train_accs), label='argmax gain = %.1f'%best_gain, c='r')\n",
    "theoretical_gain = np.sqrt(2.0 / (1 + negative_slope ** 2)) # [TO-DO]\n",
    "plt.scatter(theoretical_gain, max(train_accs), label='theoretical gain = %.2f'%theoretical_gain, c='g')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\r\n",
    "[Overcomplete MLPs are good (make link to neural tangent kernels), show how the infinite width limit produces beautifully smooth interpolations]\r\n",
    "\r\n",
    "[High dimensional spaces intuition, the idea of ensemble methods, See how mixing multiple models often helps. Conceptualize ANNs as many models in parallel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Video 17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feedback\r\n",
    "how could this session have been better? How happy are you in your group? How do you feel right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report to Airtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homeworks\r\n",
    "* (1) Join the Kaggle Competition to solve Animal Faces with random permutations using MLPs.\r\n",
    "* (2) Something where you debug something: Cross-entropy optimization with poor initialization, producing NaNs.\r\n",
    "* (3) Something related to ethics: A classification system with interest bias?\r\n",
    "* (4) Read some cool original paper:  Kernel vs Rich regimes paper?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition\r\n",
    "https://www.kaggle.com/c/permuted-animal-faces/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing the Kaggle dataset by permuting the animal faces\r\n",
    "\r\n",
    "[ To-be removed in the end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasets\n",
    "train_transform = transforms.Compose([\n",
    "     transforms.RandomRotation(10),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     ])\n",
    "\n",
    "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
    "img_train_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "     transforms.RandomRotation(10),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     ])\n",
    "img_test_dataset = ImageFolder(data_path/'val', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_perm = np.random.permutation(3*32*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"rand_perm.txt\", \"w\") as fp:\n",
    "  json.dump(rand_perm.tolist(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.zeros((len(img_test_dataset), 1+3*32*32),\n",
    "                     dtype=np.uint8)\n",
    "for i, data in enumerate(img_test_dataset):\n",
    "  X, y = data\n",
    "  X = np.array(X, dtype=np.uint8).reshape(-1)\n",
    "  X = X[rand_perm]\n",
    "  test_data[i, 0] = y\n",
    "  test_data[i, 1:] = X\n",
    "test_data = np.random.permutation(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "for i in range(3*32*32):\n",
    "  header += ['pixel%d'%(i+1)]\n",
    "\n",
    "import csv\n",
    "with open('test.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "      data = test_data[i, 1:].tolist()\n",
    "      writer.writerow(data)\n",
    "\n",
    "header = ['ImageId', 'Label']\n",
    "with open('solution.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "      data = [i+1] + [test_data[i, 0]]\n",
    "      writer.writerow(data)\n",
    "\n",
    "header = ['ImageId', 'Label']\n",
    "with open('sampleSubmission.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "      writer.writerow([i+1] + [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.zeros((len(img_train_dataset), 1+3*32*32),\n",
    "                     dtype=np.uint8)\n",
    "for i, data in enumerate(img_train_dataset):\n",
    "  X, y = data\n",
    "  X = np.array(X, dtype=np.uint8).reshape(-1)\n",
    "  X = X[rand_perm]\n",
    "  train_data[i, 0] = y\n",
    "  train_data[i, 1:] = X\n",
    "train_data = np.random.permutation(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Label\"]\n",
    "for i in range(3*32*32):\n",
    "  header += ['pixel %d'%(i+1)]\n",
    "\n",
    "with open('train.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for i in range(train_data.shape[0]):\n",
    "      data = train_data[i].tolist()\n",
    "      writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "W3_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}